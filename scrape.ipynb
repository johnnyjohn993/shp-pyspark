{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b296b2e4",
   "metadata": {},
   "source": [
    "#### [Notebook 2 Quick Start](https://confluence.shopee.io/x/7O0lhw)\n",
    "#### Common Use Cases and Solution Using Notebook 2\n",
    "- [Multi Kernels in Notebook 2](https://confluence.shopee.io/x/4Od2hw)\n",
    "- [Execute HDFS command](https://confluence.shopee.io/x/P5w8hw)\n",
    "- [Browse, upload and download from Data Infra S3](https://confluence.shopee.io/x/xo9Ohw)\n",
    "- [Setting up 3rd party packages for your Notebook 2](https://confluence.shopee.io/x/JZFOhw)\n",
    "- [Develop and setup user defined function or library](https://confluence.shopee.io/x/hpjehg)\n",
    "- [Execute Presto/Trino query directly in Python Kernel Notebook](https://confluence.shopee.io/x/WpMlhw)\n",
    "\n",
    "> Note: \"Python on K8S\" Kernel cannot execute Spark functions. ***If you want to use Spark, change kernel to Spark***. [How to change Kernel](https://confluence.shopee.io/x/4Od2hw#MultiKernelsinNotebook2-SetKernelwhencreatingnewNotebook2file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906cd37a-3a76-4c68-acb1-be58a621b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de0440c8-18d1-4a89-90dc-c6b2558a7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "script = spark.sql('''\n",
    "SELECT task_owner, workflow_code, task_name, exe_content as query\n",
    "FROM data_metamart.dwd_data_suite_task_instance_di_ph_view\n",
    "WHERE task_owner LIKE '%jayme%'\n",
    "  AND task_name IS NOT NULL\n",
    "  AND task_name = 'output_a_without_parcel_hub'\n",
    "  limit 1\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d31470f1-6244-42ae-a59c-03005a4ed0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "script.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "679359d7-8006-4e6e-8d6e-20b16277df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to extract standardized comments\n",
    "def extract_standard_comments(query):\n",
    "    pattern = re.compile(r'-- (\\d{1,2}/\\d{1,2}/\\d{4}) \\| ([\\w\\s]+) \\| (.+)')\n",
    "    matches = pattern.findall(query)\n",
    "    comments = []\n",
    "    for match in matches:\n",
    "        comments.append((match[0], match[1].strip(), match[2].strip()))\n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3872cbd-440f-4d8d-8666-d6780aa4379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------------------+---------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|task_owner|workflow_code   |task_name                  |date     |author        |description                                                                                                                                    |\n",
      "+----------+----------------+---------------------------+---------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|john.jayme|phbi_ops_6095616|output_a_without_parcel_hub|4/12/2024|Tristan Medina|Added and updated necessary columns for points logic                                                                                           |\n",
      "|john.jayme|phbi_ops_6095616|output_a_without_parcel_hub|4/23/2024|John Jayme    |Added colease in final_incentive column logic                                                                                                  |\n",
      "|john.jayme|phbi_ops_6095616|output_a_without_parcel_hub|5/21/2024|John Jayme    |Corrected Sr_weekly and added col in weekly_table CTEs , should use points                                                                     |\n",
      "|john.jayme|phbi_ops_6095616|output_a_without_parcel_hub|5/22/2024|John Jayme    |Added ros logic update for success and assign totals                                                                                           |\n",
      "|john.jayme|phbi_ops_6095616|output_a_without_parcel_hub|5/23/2024|John Jayme    |Updated calculation of final_incentive with flexi = 1 update                                                                                   |\n",
      "|john.jayme|phbi_ops_6095616|output_a_without_parcel_hub|5/31/2024|John Jayme    |added filter in casting CTE                                                                                                                    |\n",
      "|john.jayme|phbi_ops_6095616|output_a_without_parcel_hub|6/07/2024|John Jayme    |added ingestion special_hub_incentive and special_hub_incentive logic (column and left join) and case condition in incentive_or_flash_incentive|\n",
      "+----------+----------------+---------------------------+---------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define UDF to apply the extraction function\n",
    "extract_comments_udf = udf(lambda x: extract_standard_comments(x), StringType())\n",
    "\n",
    "# Apply the UDF and explode the results to create a new DataFrame\n",
    "comments_schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Extract comments and create new DataFrame\n",
    "comments_rdd = script.rdd.flatMap(lambda row: [(row['task_owner'], row['workflow_code'], row['task_name'], *comment) for comment in extract_standard_comments(row['query'])])\n",
    "comments_df = spark.createDataFrame(comments_rdd, schema=StructType([\n",
    "    StructField(\"task_owner\", StringType(), True),\n",
    "    StructField(\"workflow_code\", StringType(), True),\n",
    "    StructField(\"task_name\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True)\n",
    "]))\n",
    "\n",
    "\n",
    "comments_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc81921-d453-475b-a6a8-66d8d9dfaae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark (Livy)",
   "language": "python",
   "name": "k8s_spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
