{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179f102f-2458-4c73-865c-213d1b567ad5",
   "metadata": {},
   "source": [
    "#### [Notebook 2 Quick Start](https://confluence.shopee.io/x/7O0lhw)\n",
    "#### Common Use Cases and Solution Using Notebook 2\n",
    "- [Multi Kernels in Notebook 2](https://confluence.shopee.io/x/4Od2hw)\n",
    "- [Execute HDFS command](https://confluence.shopee.io/x/P5w8hw)\n",
    "- [Browse, upload and download from Data Infra S3](https://confluence.shopee.io/x/xo9Ohw)\n",
    "- [Setting up 3rd party packages for your Notebook 2](https://confluence.shopee.io/x/JZFOhw)\n",
    "- [Develop and setup user defined function or library](https://confluence.shopee.io/x/hpjehg)\n",
    "- [Execute Presto/Trino query directly in Python Kernel Notebook](https://confluence.shopee.io/x/WpMlhw)\n",
    "\n",
    "> Note: \"Python on K8S\" Kernel cannot execute Spark functions. ***If you want to use Spark, change kernel to Spark***. [How to change Kernel](https://confluence.shopee.io/x/4Od2hw#MultiKernelsinNotebook2-SetKernelwhencreatingnewNotebook2file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94fc8d5-7ea9-4366-b643-2c3d875a45e4",
   "metadata": {},
   "source": [
    "# Change Logs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d65dfebd-3f1a-4ec0-870d-5da7cf9bf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "import pandas as pd\n",
    "import json\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, DateType, FloatType,IntegerType\n",
    "# from pyspark.sql.functions import col, when, isnan, lit\n",
    "# from pyspark.sql.functions import when, isnan, lit, col\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdde8c07-5109-443a-ab52-9ce2ab11db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_creds = r\"\"\"{\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"eminent-expanse-421611\",\n",
    "  \"private_key_id\": \"74c6e3d99e8e1c04c6c99522701c1c5014c9ac3b\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCUe29pUfwwdzjY\\nlW7+tE0YPl7yuSlGUJS0RuxTXN+tlGbSqCb26uFtB8OHrITL6y8Krs6GBY+fCaqV\\nTrLBwZezTyPpARsGzZ30XCgE9MKqQmGKWnFRyIH/laeI+lf0b67j1fGd79YZrPRZ\\n70Ql7YeyHq5V1smYNa0DIfGVY5/2vWazQMzGJcN5hMUFnqt4YXI2LsQ2qIUHFhAi\\nGZfQACAHt5uKtKAAPFeGplIhSxRRFA6yQfJ8wlez+9vpFce7bl0KYV2I//qA2rkl\\nwKA7r7xQhY4oCxm2RDEBOouRYXU4z6Nr4snxxSs2G5FWNgvRbLhoJ/oZ5OWTTuN2\\nswM0UHWTAgMBAAECggEABtsUL9a7QDmhBE8SCuykSqRGmgp2n2qFEzXbH5ZANbGO\\nddeRxQDfW7tqCc+ragb36xRbMND86qj1mWNC2bNon7zduZp1dwDzdB71MhVjD511\\nB4BZjx8k8XolfCui+ARYHOj5+QLj7deQiL6FdQLcqxrDje8W69I+pNBiaTVqY2gV\\nVEwhzVOUgEPbHKpEaEerXQzPRZ2nSfqpyUW6r2otLkUfs74fpuUUrgPoiRCk2rrB\\n7AUxMC3/dqQEMPc7WOD8qEw6h5BnQtUQacoKlTgO/WCeiNanrCfAYsXJjK7eDRoy\\nPBF/D0WKMwEf8hWwt0Tq1rzfx6IpKI7hiII2iHWowQKBgQDQjkFuQhijqMM5+I4n\\no0Ea9CBiMEz5cBf3x2U/DVDn/fEFrMtNUApF+wTjn9Y8IeWLvSO36tZDCRfWtt24\\ndlGVTr5X/iHoBJO5sTgYY8iNHb1JNVbxbuHI5bxIvjkOwAYnw0q4gyzeR5L9o/4f\\niVcrYloGjEniW38vJiXz/rVGsQKBgQC2Qqc1G4mrloVcTsRIvD9VrKNAfLgwwqZ4\\nIIpjfhXfgjh1pT57YR34Per8uIQUTauqJWpERH9Qrfr2DePFfUI2c1A2dXgk0uFf\\nUc8OQD7P/ICJLoAAHB4sVdpHWnuVrmcQiEyttUUZR5cMO88eypGQxjWgmiGhBWLm\\nnmeT+YYZgwKBgD08d9PTDn8Hb4NXCw5ybgxnABc0jXV0R2VWo9DWrRnhQ2LiMLOb\\nu9h+cxoO9Y2kek0ElTdMeesnAVvnR8UA4MWRYGxfzz7cETAl7A2lRt7Ai7KyObwc\\nTal6Y0WMrR46ndQREKCKy6LcqzlUbKp/3ht7/mkLyinvqbGDZwN0l2+RAoGAJxQ7\\njeONrUDk71nkNERIwDH3/we+5pWV5AP2KEcMoZPmTzNTJlk4d9Wu4OkfSMsul5jW\\nJDUbp0zmloN75whofgxTkpm1XQ/qSunbn46e6TGWzdMultdwkMkHuGzqOrneV2ki\\nJ8zKOBVlibX5PvzoIUaekQT7WO8yuYgVZWEHI8kCgYEAt0Z7FWvCrDswPc6WlEvd\\niTgzsslVw40gPuA/L1uTIQw8jvyDn48DqGt0hOFwZl9dpmGQuO8lV2iW6fsfsJTx\\nzK1irECHNz5vl0YdQ4xXWPjHiJ/v3lVuxAN5dS5aSjSk20xkk4IJPWh7sQLQaf/p\\nwPYRgIBrkn1nKB4sU9MrVkA=\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"googlesheetapiediting@eminent-expanse-421611.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"105910484550738255462\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/googlesheetapiediting%40eminent-expanse-421611.iam.gserviceaccount.com\",\n",
    "  \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Convert the JSON string into a Python dictionary\n",
    "creds_dict = json.loads(json_creds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d06b76-b520-4757-822a-da3ee0867d72",
   "metadata": {},
   "source": [
    "## DataOps #1 - Data Generation / Sourcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55fe237-c45c-4a43-bf1d-0015f80b25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup Google Sheets access\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = Credentials.from_service_account_info(creds_dict, scopes=scope)\n",
    "client = gspread.authorize(creds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2308640-dfd8-453e-b32d-1ebc5aca6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_google_sheets_to_dataframes(sheet_client, sheet_key, sheet_names):\n",
    "    \"\"\"\n",
    "    This function processes multiple Google Sheets tabs, converts them into pandas DataFrames, \n",
    "    and applies a transformation to clean up the data and column headers.\n",
    "\n",
    "    Parameters:\n",
    "    sheet_client: The authenticated Google Sheets client object\n",
    "    sheet_key: The Google Sheets key (ID of the spreadsheet)\n",
    "    sheet_names: A list of sheet (tab) names to process\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of transformed DataFrames where keys are sheet names, and values are the transformed DataFrames.\n",
    "    \"\"\"\n",
    "    # Open the Google Sheet using the sheet key\n",
    "    spx_lm_master_sheet = sheet_client.open_by_key(sheet_key)\n",
    "    \n",
    "    # Dictionary to store DataFrames for each sheet\n",
    "    dfs = {}\n",
    "\n",
    "    # Loop through each sheet name\n",
    "    for sheet_name in sheet_names:\n",
    "        # Access the sheet by name\n",
    "        worksheet = spx_lm_master_sheet.worksheet(sheet_name)\n",
    "        \n",
    "        # Get all values from the sheet\n",
    "        sheet_data = worksheet.get_all_values()\n",
    "        \n",
    "        # Convert the data to a DataFrame and set the first row as column headers\n",
    "        df = pd.DataFrame(sheet_data[1:], columns=sheet_data[0])\n",
    "        \n",
    "        # Apply the transformation logic to the DataFrame\n",
    "        \n",
    "        # Strip whitespace from all string values in the DataFrame\n",
    "        df = df.apply(lambda col: col.str.strip() if col.dtype == \"object\" else col)\n",
    "        \n",
    "        # Replace empty strings with NaN\n",
    "        df.replace('', np.nan, inplace=True)\n",
    "        \n",
    "        # Strip whitespace from the column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Replace any empty column names with \"Unnamed_<index>\"\n",
    "        df.columns = [f\"unnamed_{i}\" if col == \"\" else col for i, col in enumerate(df.columns)]\n",
    "        \n",
    "        # Store the transformed DataFrame in the dictionary with the sheet name as the key\n",
    "        dfs[sheet_name] = df\n",
    "\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "429aa034-b875-403d-bcbd-fbfb85e9e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "sheet_key = '1m6MC1m4BQZ6DAqTW0GjnvWkbsRylEmyDsZ_jja1mf9s'  # Your Google Sheet key\n",
    "sheet_names = ['Ingestion_4WL_Penalty_Exemption', 'Ingestion_4WL_Penalty_Bau']  # List of sheet names\n",
    "\n",
    "# Assuming `client` is your authenticated Google Sheets client object\n",
    "transformed_dfs = process_google_sheets_to_dataframes(client, sheet_key, sheet_names)\n",
    "\n",
    "# Access a transformed DataFrame\n",
    "penalty_sheet_df = transformed_dfs['Ingestion_4WL_Penalty_Bau']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85ec4ec3-eef3-4435-a8e1-d638ae7b45a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 217 entries, 0 to 216\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   region             217 non-null    object \n",
      " 1   hub                217 non-null    object \n",
      " 2   fleet_type         217 non-null    object \n",
      " 3   driver_group       217 non-null    object \n",
      " 4   start_date         217 non-null    object \n",
      " 5   end_date           217 non-null    object \n",
      " 6   lower_limit        217 non-null    object \n",
      " 7   upper_limit        217 non-null    object \n",
      " 8   penalty            217 non-null    object \n",
      " 9   remarks            201 non-null    object \n",
      " 10  reason             217 non-null    object \n",
      " 11  vehicle_variation  217 non-null    object \n",
      " 12  min_delivery       0 non-null      float64\n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 22.2+ KB"
     ]
    }
   ],
   "source": [
    "transformed_dfs['Ingestion_4WL_Penalty_Exemption'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d682b1d7-8dff-47c5-9ed3-c80858de2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         region       hub fleet_type  ... reason vehicle_variation unnamed_12\n",
      "0  All Regions   MB Hubs        4WH  ...    NaN         Agency,IC        NaN\n",
      "1  All Regions  All hubs        4WH  ...    NaN         Agency,IC        100\n",
      "2  All Regions  All hubs        4WH  ...    NaN         Agency,IC        NaN\n",
      "3  All Regions  All hubs        4WH  ...    NaN         Agency,IC        NaN\n",
      "\n",
      "[4 rows x 13 columns]>"
     ]
    }
   ],
   "source": [
    "penalty_sheet_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0fedae7-5f6b-4efb-9988-84cff4ce5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType,DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "def infer_spark_type(dtype):\n",
    "    \"\"\"\n",
    "    Maps pandas dtype to PySpark types.\n",
    "    \"\"\"\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return IntegerType()\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return DoubleType()\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return BooleanType()\n",
    "    else:\n",
    "        return StringType()\n",
    "\n",
    "def convert_to_spark_dataframes(dfs, spark, schema_dict=None):\n",
    "    \"\"\"\n",
    "    This function converts a dictionary of pandas DataFrames into PySpark DataFrames\n",
    "    with dynamic schema generation based on the content of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dfs: Dictionary of pandas DataFrames where keys are sheet names, and values are pandas DataFrames.\n",
    "    spark: The Spark session object.\n",
    "    schema_dict: Optional dictionary where keys are sheet names and values are dictionaries \n",
    "                 that map column names to PySpark types as strings. Example: {'column1': 'string', 'column2': 'int'}\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of PySpark DataFrames where keys are sheet names, and values are the transformed PySpark DataFrames.\n",
    "    \"\"\"\n",
    "    spark_dfs = {}\n",
    "    type_mapping = {\n",
    "        'string': StringType(),\n",
    "        'int': IntegerType(),\n",
    "        'float': FloatType(),\n",
    "        'bool': BooleanType(),\n",
    "        'double':DoubleType ()\n",
    "    }\n",
    "\n",
    "    for sheet_name, df in dfs.items():\n",
    "        # Check if a custom schema is provided for this sheet, otherwise infer from DataFrame\n",
    "        if schema_dict and sheet_name in schema_dict:\n",
    "            schema = StructType([\n",
    "                StructField(col, type_mapping.get(schema_dict[sheet_name].get(col, 'string'), StringType()), True)\n",
    "                for col in df.columns\n",
    "            ])\n",
    "        else:\n",
    "            schema = StructType([\n",
    "                StructField(col, infer_spark_type(df[col].dtype), True) for col in df.columns\n",
    "            ])\n",
    "        \n",
    "        # Convert the pandas DataFrame to PySpark DataFrame using the schema\n",
    "        spark_df = spark.createDataFrame(df, schema=schema)\n",
    "        \n",
    "        # Store the PySpark DataFrame in the dictionary\n",
    "        spark_dfs[sheet_name] = spark_df\n",
    "\n",
    "    return spark_dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4aaf559-9494-437b-9cb6-b2d82fb0fc9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkStatementException",
     "evalue": "field lower_limit: DoubleType can not accept object '0.0000' in type <class 'str'>\nTraceback (most recent call last):\n  File \"<stdin>\", line 55, in convert_to_spark_dataframes\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/session.py\", line 701, in createDataFrame\n    return super(SparkSession, self).createDataFrame(\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/pandas/conversion.py\", line 340, in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/session.py\", line 728, in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/session.py\", line 526, in _createFromLocal\n    data = list(data)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/session.py\", line 710, in prepare\n    verify_func(obj)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field lower_limit: DoubleType can not accept object '0.0000' in type <class 'str'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkStatementException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Example usage:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Assuming you have already processed the Google Sheets into pandas DataFrames using the previous method\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtransformed_dfs = process_google_sheets_to_dataframes(client, sheet_key, sheet_names)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Example schema dictionary, specify the schema for specific sheets\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mschema_dict = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIngestion_4WL_Penalty_Exemption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlower_limit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdouble\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    },\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # Define more specific schemas for other sheets here\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Convert the pandas DataFrames to PySpark DataFrames\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mspark_dfs = convert_to_spark_dataframes(transformed_dfs, spark, schema_dict)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Access the PySpark DataFrame for a specific sheet\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpenalty_sheet_spark_df = spark_dfs[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIngestion_4WL_Penalty_Exemption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Show the PySpark DataFrame\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpenalty_sheet_spark_df.show()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2430\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2429\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2430\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2432\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2433\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparkmagic/livyclientlib/exceptions.py:172\u001b[0m, in \u001b[0;36mwrap_unexpected_exceptions.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mall_errors_are_fatal():\n\u001b[0;32m--> 172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(\u001b[38;5;28mself\u001b[39m, err)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparkmagic/livyclientlib/exceptions.py:169\u001b[0m, in \u001b[0;36mwrap_unexpected_exceptions.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mall_errors_are_fatal():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparkmagic/livyclientlib/exceptions.py:133\u001b[0m, in \u001b[0;36mhandle_expected_exceptions.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions_to_handle \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mall_errors_are_fatal():\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Do not log! as some messages may contain private client information\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipython_display\u001b[38;5;241m.\u001b[39msend_error(EXPECTED_ERROR_MSG\u001b[38;5;241m.\u001b[39mformat(err))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparkmagic/livyclientlib/exceptions.py:130\u001b[0m, in \u001b[0;36mhandle_expected_exceptions.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions_to_handle \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mall_errors_are_fatal():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparkmagic/kernels/kernelmagics.py:369\u001b[0m, in \u001b[0;36mKernelMagics.spark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_not_call_start_session(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_with_code_kind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSESSION_KIND_PYSPARK\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparkmagic/kernels/kernelmagics.py:416\u001b[0m, in \u001b[0;36mKernelMagics._run_with_code_kind\u001b[0;34m(self, line, cell, local_ns, code_kind)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark_common_execute(line, cell, local_ns)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     session\u001b[38;5;241m.\u001b[39mcode_kind \u001b[38;5;241m=\u001b[39m origin_language\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparkmagic/kernels/kernelmagics.py:414\u001b[0m, in \u001b[0;36mKernelMagics._run_with_code_kind\u001b[0;34m(self, line, cell, local_ns, code_kind)\u001b[0m\n\u001b[1;32m    412\u001b[0m session\u001b[38;5;241m.\u001b[39mcode_kind \u001b[38;5;241m=\u001b[39m code_kind\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark_common_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparkmagic/kernels/kernelmagics.py:378\u001b[0m, in \u001b[0;36mKernelMagics.spark_common_execute\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    374\u001b[0m args \u001b[38;5;241m=\u001b[39m parse_argstring_or_throw(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark, line)\n\u001b[1;32m    376\u001b[0m coerce \u001b[38;5;241m=\u001b[39m get_coerce_value(args\u001b[38;5;241m.\u001b[39mcoerce)\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_spark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamplemethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamplefraction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoerce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sparkmagic/magics/sparkmagicsbase.py:143\u001b[0m, in \u001b[0;36mSparkMagicBase.execute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce, output_handler)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mshutdown_session_on_spark_statement_errors():\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark_controller\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkStatementException(out)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, string_types):\n",
      "\u001b[0;31mSparkStatementException\u001b[0m: field lower_limit: DoubleType can not accept object '0.0000' in type <class 'str'>\nTraceback (most recent call last):\n  File \"<stdin>\", line 55, in convert_to_spark_dataframes\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/session.py\", line 701, in createDataFrame\n    return super(SparkSession, self).createDataFrame(\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/pandas/conversion.py\", line 340, in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/session.py\", line 728, in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/session.py\", line 526, in _createFromLocal\n    data = list(data)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/session.py\", line 710, in prepare\n    verify_func(obj)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/mnt/ssd/0/yarn/nm-local-dir/usercache/john.jayme/appcache/application_1727161546827_6191304/container_e113_1727161546827_6191304_01_000001/pyspark.zip/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field lower_limit: DoubleType can not accept object '0.0000' in type <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "# Assuming you have already processed the Google Sheets into pandas DataFrames using the previous method\n",
    "transformed_dfs = process_google_sheets_to_dataframes(client, sheet_key, sheet_names)\n",
    "\n",
    "# Example schema dictionary, specify the schema for specific sheets\n",
    "schema_dict = {\n",
    "    'Ingestion_4WL_Penalty_Exemption': {\n",
    "        'lower_limit': 'double'\n",
    "    },\n",
    "    # Define more specific schemas for other sheets here\n",
    "}\n",
    "\n",
    "# Convert the pandas DataFrames to PySpark DataFrames\n",
    "spark_dfs = convert_to_spark_dataframes(transformed_dfs, spark, schema_dict)\n",
    "\n",
    "# Access the PySpark DataFrame for a specific sheet\n",
    "penalty_sheet_spark_df = spark_dfs['Ingestion_4WL_Penalty_Exemption']\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "penalty_sheet_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94a9921e-48fc-4f86-9748-935104dac08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----------+--------------------+----------+----------+-----------+-----------+--------+-----------------+--------------------+-----------------+------------+\n",
      "|            region|                 hub|fleet_type|        driver_group|start_date|  end_date|lower_limit|upper_limit| penalty|          remarks|              reason|vehicle_variation|min_delivery|\n",
      "+------------------+--------------------+----------+--------------------+----------+----------+-----------+-----------+--------+-----------------+--------------------+-----------------+------------+\n",
      "| Central Visayas 1|         Banilad Hub|       4WH|       [LM] Delivery|2024-02-07|2024-02-09|     0.0000|     0.0000|   0.000|              NaN|     just an example|        Agency,IC|         NaN|\n",
      "|    South Mindanao|            All Hubs|       4WH|       [LM] Delivery|2024-01-15|2024-01-21|     0.0000|     0.0000|   0.000|              NaN|Bonus/Penalty Rel...|        Agency,IC|         NaN|\n",
      "|    South Mindanao|            All Hubs|       4WH|       [LM] Delivery|2024-01-29|2024-02-04|     0.0000|     0.0000|   0.000|              NaN|4WL Bonus/Penalty...|        Agency,IC|         NaN|\n",
      "|North West Luzon 2|          Baguio Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|North West Luzon 2|   BAG-Magsaysay Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|North West Luzon 2|          Irisan Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|North West Luzon 2|La Trinidad-Pico Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|North West Luzon 2| Baguio Bakakeng Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|        Cotabato Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|           Davao Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|         Bangkal Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|       Boulevard Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|        Midsayap Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|        Tacurong Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 1|            All hubs|       4WH|         [LM] Feeder|2024-01-29|2024-04-14|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 2|            All hubs|       4WH|         [LM] Feeder|2024-01-29|2024-04-14|     0.0000|     0.0000|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 2|        San Juan Hub|       4WH|[LM] Delivery,[LM...|2024-06-21|2024-07-06|     0.0000|    80.0000|   0.000|       no penalty|      No 4WL Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 1|         Palanca Hub|       4WH|[LM] Delivery,[LM...|2024-06-21|2024-06-27|     0.0000|    80.0000|   0.000|       no penalty|      No 4WL Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 1|            All hubs|       4WH|[LM] Delivery,[LM...|2024-07-25|2024-07-26|     0.0000|    40.0000|-200.000|relaxed threshold|             typhoon|        Agency,IC|         NaN|\n",
      "|    Metro Manila 2|            All hubs|       4WH|[LM] Delivery,[LM...|2024-07-25|2024-07-26|     0.0000|    40.0000|-200.000|relaxed threshold|             typhoon|        Agency,IC|         NaN|\n",
      "+------------------+--------------------+----------+--------------------+----------+----------+-----------+-----------+--------+-----------------+--------------------+-----------------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming dfs is your dictionary of pandas DataFrames and schema_dict is your column mapping\n",
    "def cast_columns_to_double(df, schema_dict):\n",
    "    \"\"\"\n",
    "    Casts columns to double where applicable based on the schema dictionary.\n",
    "    \"\"\"\n",
    "    for column_name, column_type in schema_dict.items():\n",
    "        if column_type == 'double':\n",
    "            # Cast the column to DoubleType\n",
    "            df = df.withColumn(column_name, col(column_name).cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have already processed the Google Sheets into pandas DataFrames using the previous method\n",
    "transformed_dfs = process_google_sheets_to_dataframes(client, sheet_key, sheet_names)\n",
    "\n",
    "# Example schema dictionary, specify the schema for specific sheets\n",
    "schema_dict = {\n",
    "    'Ingestion_4WL_Penalty_Exemption': {\n",
    "        'lower_limit': 'string'\n",
    "    },\n",
    "    # Define more specific schemas for other sheets here\n",
    "}\n",
    "\n",
    "# Convert the pandas DataFrames to PySpark DataFrames\n",
    "spark_dfs = convert_to_spark_dataframes(transformed_dfs, spark, schema_dict)\n",
    "\n",
    "# Access the PySpark DataFrame for a specific sheet\n",
    "penalty_sheet_spark_df = spark_dfs['Ingestion_4WL_Penalty_Exemption']\n",
    "\n",
    "# Apply the casting function\n",
    "penalty_sheet_spark_df = cast_columns_to_double(penalty_sheet_spark_df, schema_dict['Ingestion_4WL_Penalty_Exemption'])\n",
    "\n",
    "# Show the PySpark DataFrame with the properly cast column\n",
    "penalty_sheet_spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0399ba0-00d4-453c-a617-69344441f03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- region: string (nullable = true)\n",
      " |-- hub: string (nullable = true)\n",
      " |-- fleet_type: string (nullable = true)\n",
      " |-- driver_group: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- lower_limit: string (nullable = true)\n",
      " |-- upper_limit: string (nullable = true)\n",
      " |-- penalty: string (nullable = true)\n",
      " |-- remarks: string (nullable = true)\n",
      " |-- reason: string (nullable = true)\n",
      " |-- vehicle_variation: string (nullable = true)\n",
      " |-- min_delivery: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "penalty_sheet_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bf7fe6e-9029-4777-bce1-02bd63cb9740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----------+--------------------+----------+----------+-----------+-----------+--------+-----------------+--------------------+-----------------+------------+\n",
      "|            region|                 hub|fleet_type|        driver_group|start_date|  end_date|lower_limit|upper_limit| penalty|          remarks|              reason|vehicle_variation|min_delivery|\n",
      "+------------------+--------------------+----------+--------------------+----------+----------+-----------+-----------+--------+-----------------+--------------------+-----------------+------------+\n",
      "| Central Visayas 1|         Banilad Hub|       4WH|       [LM] Delivery|2024-02-07|2024-02-09|        0.0|          0|   0.000|              NaN|     just an example|        Agency,IC|         NaN|\n",
      "|    South Mindanao|            All Hubs|       4WH|       [LM] Delivery|2024-01-15|2024-01-21|        0.0|          0|   0.000|              NaN|Bonus/Penalty Rel...|        Agency,IC|         NaN|\n",
      "|    South Mindanao|            All Hubs|       4WH|       [LM] Delivery|2024-01-29|2024-02-04|        0.0|          0|   0.000|              NaN|4WL Bonus/Penalty...|        Agency,IC|         NaN|\n",
      "|North West Luzon 2|          Baguio Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|North West Luzon 2|   BAG-Magsaysay Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|North West Luzon 2|          Irisan Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|North West Luzon 2|La Trinidad-Pico Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|North West Luzon 2| Baguio Bakakeng Hub|       4WH|       [LM] Delivery|2024-02-24|2024-02-25|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|        Cotabato Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|           Davao Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|         Bangkal Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|       Boulevard Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|        Midsayap Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    South Mindanao|        Tacurong Hub|       4WH|[LM] Delivery, [L...|2024-04-01|2024-04-11|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 1|            All hubs|       4WH|         [LM] Feeder|2024-01-29|2024-04-14|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 2|            All hubs|       4WH|         [LM] Feeder|2024-01-29|2024-04-14|        0.0|          0|   0.000|              NaN|No 4WL Bonus Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 2|        San Juan Hub|       4WH|[LM] Delivery,[LM...|2024-06-21|2024-07-06|        0.0|         80|   0.000|       no penalty|      No 4WL Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 1|         Palanca Hub|       4WH|[LM] Delivery,[LM...|2024-06-21|2024-06-27|        0.0|         80|   0.000|       no penalty|      No 4WL Penalty|        Agency,IC|         NaN|\n",
      "|    Metro Manila 1|            All hubs|       4WH|[LM] Delivery,[LM...|2024-07-25|2024-07-26|        0.0|         40|-200.000|relaxed threshold|             typhoon|        Agency,IC|         NaN|\n",
      "|    Metro Manila 2|            All hubs|       4WH|[LM] Delivery,[LM...|2024-07-25|2024-07-26|        0.0|         40|-200.000|relaxed threshold|             typhoon|        Agency,IC|         NaN|\n",
      "+------------------+--------------------+----------+--------------------+----------+----------+-----------+-----------+--------+-----------------+--------------------+-----------------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, DoubleType, IntegerType, BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Type mapping dictionary for custom casting\n",
    "type_mapping = {\n",
    "    'string': StringType(),\n",
    "    'int': IntegerType(),\n",
    "    'double': DoubleType(),\n",
    "    'bool': BooleanType()\n",
    "}\n",
    "\n",
    "def cast_columns_with_default(df, schema_dict):\n",
    "    \"\"\"\n",
    "    Casts specified columns to the types mentioned in the schema dictionary, while retaining\n",
    "    all other columns as strings.\n",
    "    \n",
    "    Parameters:\n",
    "    df: The PySpark DataFrame to apply the casting to.\n",
    "    schema_dict: A dictionary where the keys are column names and values are the types to cast ('int', 'double', etc.).\n",
    "    \n",
    "    Returns:\n",
    "    The PySpark DataFrame with the specified columns cast to the desired types and all other columns as strings.\n",
    "    \"\"\"\n",
    "    for column_name in df.columns:\n",
    "        # Check if the column is in the schema_dict for custom casting\n",
    "        if column_name in schema_dict:\n",
    "            cast_type = schema_dict[column_name]\n",
    "            if cast_type in type_mapping:\n",
    "                df = df.withColumn(column_name, col(column_name).cast(type_mapping[cast_type]))\n",
    "        else:\n",
    "            # Retain as string by default for columns not listed in schema_dict\n",
    "            df = df.withColumn(column_name, col(column_name).cast(StringType()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have already processed the Google Sheets into pandas DataFrames using the previous method\n",
    "transformed_dfs = process_google_sheets_to_dataframes(client, sheet_key, sheet_names)\n",
    "\n",
    "# Example schema dictionary, specify the schema for specific columns\n",
    "schema_dict = {\n",
    "    'lower_limit': 'double',  # Specify the columns to cast\n",
    "    'upper_limit': 'int'\n",
    "}\n",
    "\n",
    "# Convert the pandas DataFrames to PySpark DataFrames\n",
    "spark_dfs = convert_to_spark_dataframes(transformed_dfs, spark)\n",
    "\n",
    "# Access the PySpark DataFrame for a specific sheet\n",
    "penalty_sheet_spark_df = spark_dfs['Ingestion_4WL_Penalty_Exemption']\n",
    "\n",
    "# Apply the casting method with defaults\n",
    "penalty_sheet_spark_df = cast_columns_with_default(penalty_sheet_spark_df, schema_dict)\n",
    "\n",
    "# Show the resulting DataFrame with custom and default casting\n",
    "penalty_sheet_spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c36a18e-9bb6-4dc5-86ee-3f06bc376068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- region: string (nullable = true)\n",
      " |-- hub: string (nullable = true)\n",
      " |-- fleet_type: string (nullable = true)\n",
      " |-- driver_group: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- lower_limit: double (nullable = true)\n",
      " |-- upper_limit: integer (nullable = true)\n",
      " |-- penalty: string (nullable = true)\n",
      " |-- remarks: string (nullable = true)\n",
      " |-- reason: string (nullable = true)\n",
      " |-- vehicle_variation: string (nullable = true)\n",
      " |-- min_delivery: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "penalty_sheet_spark_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark (Livy)",
   "language": "python",
   "name": "k8s_spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
